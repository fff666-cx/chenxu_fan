# chenxu_fan
学习笔记以及求解过程
#1.第一问RNN完成对FashionMnist数据集的分类
##1.1RNN网络的建立：
使用nn.Linear进行初步的搭建，并进行数据初始化。设置学习轮数为5次，使用Adam作为优化器，对数据进行学习并统计交叉熵损失后，损失一直较大，最低在0.7左右，故考虑对模型进行改进。（代码如"RNN（first）.py"）
##1.2权重矩阵初始化的改进：
在可能对结果有影响的几个因素中考虑改进，学习率已有Adam优化器进行自动更新。而权重矩阵的初始化在nn.Linear中是正态分布进行初始化，经过查找资料，对于我使用的tanh函数做激活函数时，使用 Xavier 初始化更有利于降低损失值，故在"RNN（with early stop）.py"添加模块对权重矩阵重新进行初始化。
##1.3对学习轮数的改进：
初始学习次数定为5，效果不是很理想，让我觉得可能是学习次数过少导致欠拟合。在经过查找资料后决定使用早停机制（early stopping）进行改进，最开始设置patience值为2，结果与原来差距不大。故增大到5，delta设置为0.01。
##1.4模型训练与可视化：
取数据集的0.8作为训练集，0.2作为测试集，准确率提升至83%至85%左右。训练过程见（RNN.txt),最后随机选取一批图像和对应标签输出，可视化结果。
##1.5问题一的总结和反思：
###1.5.1学习了解了RNN的基本模型架构，学到了降低损失，提高准确率的几个努力的方向和方法。
###1.5.2对early stopping的使用暂不熟悉，可能导致patience值设置过大导致过拟合。
#2.第二问位置编码和旋转位置编码
##2.1引入位置编码的原因和必要性：
我在学习途中了解到transformer的核心算法attention，在对词向量进行attention运算后，不能体现相邻词的互相影响，如"Are you ok?"与"Are ok? you"对应的attention结果一致，这与想达到的效果不符合，于是引入位置编码，将输入的特征向量变为“词向量＋位置编码”，而这一点改变在深度学习庞大的数据训练后，模型能够分辨位置这个信息。
##2.2位置编码和旋转位置编码的实现：
见（PE.py）与（ROPE.py）
##2.3位置编码和旋转位置编码的区别：
ROPE会在注意力机制中直接应用旋转变换，以便保持位置的相对性，而位置编码是固定的。在处理长序列时，旋转位置编码能够更好的捕捉相对位置的信息。
#3.多头注意力机制
##3.1注意力机制：
注意力机制的建立让模型在处理时关注到整个序列的每一个词，从而更好地理解上下文，将对所有词的理解融入正在处理的词。第一步：将输入的词向量x乘上Q,V,K对应的权重值，得到Q,V,K矩阵。第二步：通过Q与K做向量点积，得到打分值。第三步：除根号dim，做softmax，得到概率值，概率值越大意味着两个词关联越大。第四步：乘上V得到z，将z向前传播。
##3.2多头注意力机制：
分为多个头（权重）对数据进行注意力运算，扩展了专注于不同位置的能力。（只有第0个编码器需要进行词嵌入，其余的编码器接受上个编码器输出的结果），最后将所有z结合在一起乘权重矩阵W0后向前传播。
#4.附加题
##4.1DDPM的理解：
通过学习训练数据的分布，产出尽可能符合训练数据分布的真实图片。学习训练数据的过程称为Diffusion，逐步为训练图片加上噪声，每次加噪声都要重新取样有些许麻烦，故通过重参数的方法，直接把x0变成xT。再通过反向的Denoise进行逐步的降噪。经过复杂的数学推导，可以得到均值与噪声的关系，于是通过预测噪声来预测均值，达到让输出的图片和真实图片接近的目的。
##4.2DDPM的实现：
